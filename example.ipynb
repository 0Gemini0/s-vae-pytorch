{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "from hyperspherical_vae.distributions import VonMisesFisher\n",
    "from hyperspherical_vae.distributions import HypersphericalUniform\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=True, download=True,\n",
    "    transform=transforms.ToTensor()), batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=False, download=True,\n",
    "    transform=transforms.ToTensor()), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVAE(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, h_dim, z_dim, activation=F.relu, distribution='normal'):\n",
    "        \"\"\"\n",
    "        ModelVAE initializer\n",
    "        :param h_dim: dimension of the hidden layers\n",
    "        :param z_dim: dimension of the latent representation\n",
    "        :param activation: callable activation function\n",
    "        :param distribution: string either `normal` or `vmf`, indicates which distribution to use\n",
    "        \"\"\"\n",
    "        super(ModelVAE, self).__init__()\n",
    "        \n",
    "        self.z_dim, self.activation, self.distribution = z_dim, activation, distribution\n",
    "        \n",
    "        # 2 hidden layers encoder\n",
    "        self.fc_e0 = nn.Linear(784, h_dim * 2)\n",
    "        self.fc_e1 = nn.Linear(h_dim * 2, h_dim)\n",
    "\n",
    "        if self.distribution == 'normal':\n",
    "            # compute mean and std of the normal distribution\n",
    "            self.fc_mean = nn.Linear(h_dim, z_dim)\n",
    "            self.fc_var =  nn.Linear(h_dim, z_dim)\n",
    "        elif self.distribution == 'vmf':\n",
    "            # compute mean and concentration of the von Mises-Fisher\n",
    "            self.fc_mean = nn.Linear(h_dim, z_dim)\n",
    "            self.fc_var = nn.Linear(h_dim, 1)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "            \n",
    "        # 2 hidden layers decoder\n",
    "        self.fc_d0 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc_d1 = nn.Linear(h_dim, h_dim * 2)\n",
    "        self.fc_logits = nn.Linear(h_dim * 2, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # 2 hidden layers encoder\n",
    "        x = self.activation(self.fc_e0(x))\n",
    "        x = self.activation(self.fc_e1(x))\n",
    "        \n",
    "        if self.distribution == 'normal':\n",
    "            # compute mean and std of the normal distribution\n",
    "            z_mean = self.fc_mean(x)\n",
    "            z_var = F.softplus(self.fc_var(x))\n",
    "        elif self.distribution == 'vmf':\n",
    "            # compute mean and concentration of the von Mises-Fisher\n",
    "            z_mean = self.fc_mean(x)\n",
    "            z_mean /= z_mean.norm(dim=-1, keepdim=True)\n",
    "            # the `+ 1` prevent collapsing behaviors\n",
    "            z_var = F.softplus(self.fc_var(x)) + 1\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        \n",
    "        return z_mean, z_var\n",
    "        \n",
    "    def decode(self, z):\n",
    "        \n",
    "        x = self.activation(self.fc_d0(z))\n",
    "        x = self.activation(self.fc_d1(x))\n",
    "        x = self.fc_logits(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def reparameterize(self, z_mean, z_var):\n",
    "        if self.distribution == 'normal':\n",
    "            q_z = torch.distributions.normal.Normal(z_mean, z_var)\n",
    "            p_z = torch.distributions.normal.Normal(torch.zeros_like(z_mean), torch.ones_like(z_var))\n",
    "        elif self.distribution == 'vmf':\n",
    "            q_z = VonMisesFisher(z_mean, z_var)\n",
    "            p_z = HypersphericalUniform(self.z_dim - 1)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return q_z, p_z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # dynamic binarization\n",
    "#         x = tf.cast(tf.greater(x, tf.random_uniform(shape=tf.shape(x), dtype=x.dtype)), dtype=x.dtype)\n",
    "        \n",
    "        z_mean, z_var = self.encode(x)\n",
    "        q_z, p_z = self.reparameterize(z_mean, z_var)\n",
    "        z = q_z.rsample()\n",
    "        x_ = self.decode(z)\n",
    "        \n",
    "        return (z_mean, z_var), (q_z, p_z), z, x_\n",
    "    \n",
    "    \n",
    "# def log_likelihood(model, x, n=10):\n",
    "#     \"\"\"\n",
    "#     :param model: model object\n",
    "#     :param optimizer: optimizer object\n",
    "#     :param n: number of MC samples\n",
    "#     :return: MC estimate of log-likelihood\n",
    "#     \"\"\"\n",
    "\n",
    "#     z_mean, z_var = model.encode(x.reshape(-1, 784))\n",
    "#     q_z, p_z = model.reparameterize(z_mean, z_var)\n",
    "#     z = q_z.rsample(torch.Size([n]))\n",
    "#     x_mb_ = model.decode(z)\n",
    "\n",
    "#     log_p_z = p_z.log_prob(z)\n",
    "\n",
    "#     if model.distribution == 'normal':\n",
    "#         log_p_z = log_p_z.sum(-1)\n",
    "\n",
    "#     log_p_x_z = -nn.BCEWithLogitsLoss(reduction='none')(x_mb_, x_mb.reshape(-1, 784).repeat((n, 1, 1))).sum(-1)\n",
    "\n",
    "#     log_q_z_x = q_z.log_prob(z)\n",
    "\n",
    "#     if model.distribution == 'normal':\n",
    "#         log_q_z_x = log_q_z_x.sum(-1)\n",
    "\n",
    "#     return ((log_p_x_z + log_p_z - log_q_z_x).t() - np.log(n)).logsumexp(-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelVAE(h_dim=128, z_dim=5, distribution='normal')\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for x_mb, y_mb in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n",
    "\n",
    "        loss_recon = nn.BCEWithLogitsLoss()(x_mb_, x_mb.reshape(-1, 784)).sum(-1).mean()\n",
    "        loss = loss_recon\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        print(loss)\n",
    "\n",
    "#     print_ = defaultdict(list)\n",
    "#     for x_mb, y_mb in test_loader:\n",
    "\n",
    "#         _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n",
    "        \n",
    "#         print_['recon loss'].append(float(nn.BCEWithLogitsLoss(reduction='none')(x_mb_, x_mb.reshape(-1, 784)).sum(-1).mean().data))\n",
    "#         print_['KL'].append(float(torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1).mean().data))\n",
    "#         print_['ELBO'].append(- print_['recon loss'][-1] - print_['KL'][-1])\n",
    "#         print_['LL'].append(float(log_likelihood(model, x_mb).data))\n",
    "    \n",
    "#     print({k: np.mean(v) for k, v in print_.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
